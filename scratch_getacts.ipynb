{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1195f81-135f-4039-8e66-65d519443006",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import configparser\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config.ini')\n",
    "LLAMA_DIRECTORY = config['LLaMA']['weights_directory']\n",
    "\n",
    "if not os.path.exists(LLAMA_DIRECTORY):\n",
    "    raise Exception(\"Make sure you've set the path to your LLaMA weights in config.ini\")\n",
    "\n",
    "class Hook:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "\n",
    "    def __call__(self, module, module_inputs, module_outputs):\n",
    "        self.out, _ = module_outputs\n",
    "\n",
    "\n",
    "def load_llama(model_size, device):\n",
    "    llama_path = os.path.join(LLAMA_DIRECTORY, config['LLaMA'][f'{model_size}_subdir'])\n",
    "    print(llama_path)\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(llama_path, device_map='cuda')\n",
    "    model = LlamaForCausalLM.from_pretrained(llama_path, device_map='cuda')\n",
    "    # set tokenizer to use bos token\n",
    "    tokenizer.bos_token = '<s>'\n",
    "    # if model_size == '13B' and device != 'cpu':\n",
    "    #     model = model.half()\n",
    "    model.to(device)\n",
    "    return tokenizer, model\n",
    "\n",
    "def load_statements(dataset_name):\n",
    "    \"\"\"\n",
    "    Load statements from csv file, return list of strings.\n",
    "    \"\"\"\n",
    "    dataset = pd.read_csv(f\"datasets/{dataset_name}.csv\")\n",
    "    statements = dataset['statement'].tolist()\n",
    "    return statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07fc0197-5ab3-40f5-bc05-c308469ff8dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/storage/models/llama-2-13B-GPTQ-4bit\n"
     ]
    }
   ],
   "source": [
    "tokenizer, model = load_llama('13B-2', 'cuda:0')\n",
    "all_statements = load_statements('cities')\n",
    "device='cuda:0'\n",
    "layers=[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0455e781-1e0c-4dc1-aa85-8b3853e4e8fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.00it/s]\n"
     ]
    }
   ],
   "source": [
    "statements=all_statements[:1]\n",
    "\n",
    "# attach hooks\n",
    "hooks, handles = [], []\n",
    "for layer in layers:\n",
    "    hook = Hook()\n",
    "    handle = model.model.layers[layer].register_forward_hook(hook)\n",
    "    hooks.append(hook), handles.append(handle)\n",
    "\n",
    "# get activations\n",
    "acts = {layer : [] for layer in layers}\n",
    "for statement in tqdm(statements):\n",
    "    input_ids = tokenizer.encode(statement, return_tensors=\"pt\").to(device)\n",
    "    model(input_ids)\n",
    "    for layer, hook in zip(layers, hooks):\n",
    "        acts[layer].append(hook.out[0, -1])\n",
    "\n",
    "for layer, act in acts.items():\n",
    "    acts[layer] = t.stack(act).float()\n",
    "\n",
    "# remove hooks\n",
    "for handle in handles:\n",
    "    handle.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c9b51aa-25f4-45d0-a155-e44435d3dfc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0133, -0.0036,  0.0015,  ...,  0.0061,  0.0028,  0.0415],\n",
       "       device='cuda:0', grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acts[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfb9d677-99c0-4c5e-8856-41b74d951471",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1,   450,  4272,   310,   476,  3417, 29876,   397,   279,   338,\n",
       "           297, 12710, 29889]], device='cuda:0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "236351e6-f330-47c4-86d4-4b81ea2e3c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs=model(input_ids, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2da33cd7-9ebc-4cce-99e9-3f10bc5af1cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0133, -0.0036,  0.0015,  ...,  0.0061,  0.0028,  0.0415],\n",
       "       device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.hidden_states[1][0][-1][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "33b54d4a-8f75-4f4b-8178-108cab098827",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2035, -0.4111,  0.2014,  ..., -0.0191,  0.3267, -0.5210],\n",
       "         [-0.1396, -0.7192,  0.0668,  ..., -0.0035,  0.3130,  0.3230],\n",
       "         [-0.1270,  0.1328, -0.0320,  ...,  0.3794,  0.2050, -0.6992],\n",
       "         ...,\n",
       "         [-0.1923, -0.1631, -0.2490,  ...,  0.2705,  0.1188, -0.2795],\n",
       "         [-0.1245, -0.1892, -0.2700,  ..., -0.2158,  0.1249, -0.9170],\n",
       "         [ 0.0220, -0.2148, -0.4954,  ..., -0.1597, -0.0896, -0.1294]]],\n",
       "       device='cuda:0', dtype=torch.float16, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.hidden_states[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804da478-0ca5-4f77-9294-e7b7b4ac796a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
