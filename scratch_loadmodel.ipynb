{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90aa56ae-af3f-4517-970a-a6c446734d5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e880aef6-4a0f-4d60-8f2c-5d7889548067",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from utils import collect_acts\n",
    "from generate_acts import load_llama\n",
    "from probes import LRProbe, MMProbe, CCSProbe\n",
    "import plotly.express as px\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eef66ccb-17e0-4f88-b9a6-790f7563a53f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/storage/models/llama-2-13B-GPTQ-4bit\n"
     ]
    }
   ],
   "source": [
    "model_size = '13B-2'\n",
    "device = 'cuda:0'\n",
    "\n",
    "tokenizer, model = load_llama(model_size, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3ff672b-1759-491a-bb40-ef78a79804e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\\\n",
    "The Spanish word 'jirafa' means 'giraffe'. This statement is: TRUE\n",
    "The Spanish word 'escribir' means 'to write'. This statement is: TRUE\n",
    "The Spanish word 'diccionario' means 'dictionary'. This statement is: TRUE\n",
    "The Spanish word 'gato' means 'cat'. This statement is: TRUE\n",
    "The Spanish word 'aire' means 'silver'. This statement is: FALSE\"\"\"\n",
    "\n",
    "statement=\"The Spanish word 'hola' means 'hello'.\"\n",
    "input_ids = tokenizer(prompt + '\\n' +  statement + ' This statement is:', return_tensors='pt').input_ids.to(device)\n",
    "probs = model(input_ids).logits[0,-1,:].softmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1feb0d3-bd21-4dab-b253-e234ee1052ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.8439, device='cuda:0', grad_fn=<SelectBackward0>),\n",
       " tensor(0.1513, device='cuda:0', grad_fn=<SelectBackward0>))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_tok = tokenizer.encode('TRUE')[-1]\n",
    "f_tok = tokenizer.encode('FALSE')[-1]\n",
    "\n",
    "probs[t_tok], probs[f_tok]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f27a5b06-60e0-4ada-a5b7-7380d2b90647",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-01T18:29:15.782780Z",
     "iopub.status.busy": "2024-02-01T18:29:15.782508Z",
     "iopub.status.idle": "2024-02-01T18:29:15.915426Z",
     "shell.execute_reply": "2024-02-01T18:29:15.914588Z",
     "shell.execute_reply.started": "2024-02-01T18:29:15.782760Z"
    }
   },
   "outputs": [],
   "source": [
    "outputs = model(input_ids, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "015081c2-a658-4d92-beb1-33fbe117109f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['logits', 'past_key_values', 'hidden_states'])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c3dac216-831f-46e4-8a2b-1129ce1e7399",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(outputs.past_key_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fb0e847a-77ab-4f6a-a100-74c04aece306",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(outputs.past_key_values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ba927456-326d-4717-95be-f6bdafebc176",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 40, 107, 128])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.past_key_values[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ba9202fa-550c-43fb-91b3-01b81be4a922",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 107, 32000])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9a64b276-cce8-4faa-b249-496e7d1f03e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(outputs.hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "89b3e17c-cc92-4c04-a327-c2f48bb9cea8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 107, 5120])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.hidden_states[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "5a7c0a03-126a-4352-88b1-18f31bf42be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight torch.Size([32000, 5120])\n",
      "model.layers.0.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.0.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.1.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.1.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.2.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.2.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.3.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.3.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.4.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.4.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.5.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.5.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.6.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.6.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.7.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.7.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.8.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.8.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.9.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.9.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.10.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.10.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.11.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.11.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.12.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.12.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.13.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.13.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.14.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.14.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.15.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.15.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.16.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.16.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.17.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.17.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.18.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.18.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.19.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.19.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.20.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.20.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.21.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.21.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.22.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.22.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.23.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.23.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.24.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.24.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.25.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.25.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.26.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.26.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.27.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.27.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.28.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.28.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.29.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.29.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.30.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.30.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.31.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.31.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.32.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.32.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.33.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.33.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.34.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.34.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.35.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.35.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.36.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.36.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.37.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.37.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.38.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.38.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.39.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.39.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.norm.weight torch.Size([5120])\n",
      "lm_head.weight torch.Size([32000, 5120])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaModel(\n",
       "  (embed_tokens): Embedding(32000, 5120, padding_idx=0)\n",
       "  (layers): ModuleList(\n",
       "    (0-39): 40 x LlamaDecoderLayer(\n",
       "      (self_attn): LlamaSdpaAttention(\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "        (k_proj): QuantLinear()\n",
       "        (o_proj): QuantLinear()\n",
       "        (q_proj): QuantLinear()\n",
       "        (v_proj): QuantLinear()\n",
       "      )\n",
       "      (mlp): LlamaMLP(\n",
       "        (act_fn): SiLU()\n",
       "        (down_proj): QuantLinear()\n",
       "        (gate_proj): QuantLinear()\n",
       "        (up_proj): QuantLinear()\n",
       "      )\n",
       "      (input_layernorm): LlamaRMSNorm()\n",
       "      (post_attention_layernorm): LlamaRMSNorm()\n",
       "    )\n",
       "  )\n",
       "  (norm): LlamaRMSNorm()\n",
       ")"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.named_parameters()\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.size())\n",
    "\n",
    "# for module in model.children():\n",
    "#     print('hi')\n",
    "#     print(module)\n",
    "\n",
    "model.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "ac52a9bf-8cdd-45d8-b568-68647099d9ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0-39): 40 x LlamaDecoderLayer(\n",
       "    (self_attn): LlamaSdpaAttention(\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "      (k_proj): QuantLinear()\n",
       "      (o_proj): QuantLinear()\n",
       "      (q_proj): QuantLinear()\n",
       "      (v_proj): QuantLinear()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (act_fn): SiLU()\n",
       "      (down_proj): QuantLinear()\n",
       "      (gate_proj): QuantLinear()\n",
       "      (up_proj): QuantLinear()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm()\n",
       "    (post_attention_layernorm): LlamaRMSNorm()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e198ed4-c3e4-4e1a-8739-7af54f386557",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
